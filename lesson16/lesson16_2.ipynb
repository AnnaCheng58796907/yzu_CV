{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cd08b6c",
   "metadata": {},
   "source": [
    "#### CV_mp 虛擬環境\n",
    "* model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8008156e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import cv2\n",
    "\n",
    "base_options = python.BaseOptions(\n",
    "    model_asset_path='models/blaze_face_short_range.tflite')\n",
    "options = vision.FaceDetectorOptions(base_options=base_options)\n",
    "detector = vision.FaceDetector.create_from_options(options)\n",
    "img = mp.Image.create_from_file(\"images/face.jpg\")\n",
    "detection_result = detector.detect(img)\n",
    "if detection_result:\n",
    "    img = img.numpy_view()\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    height, width, _ = img.shape\n",
    "    for detection in detection_result.detections:\n",
    "        bbox = detection.bounding_box\n",
    "        x1 = bbox.origin_x\n",
    "        y1 = bbox.origin_y\n",
    "        x2 = bbox.origin_x + bbox.width\n",
    "        y2 = bbox.origin_y + bbox.height\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 255), 2)\n",
    "        for keypoint in detection.keypoints:\n",
    "            cx = int(keypoint.x * width)\n",
    "            cy = int(keypoint.y * height)\n",
    "            cv2.circle(img, (cx, cy), 2, (255, 0, 255), 2)\n",
    "\n",
    "        category = detection.categories[0]\n",
    "        probability = round(category.score, 2)\n",
    "        score = str(probability * 100) + \"%\"\n",
    "        cv2.putText(img, score, (x1, y1 - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "cv2.imshow(\"MediaPipe Face Detection\", img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87d8747",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mediapipe import solutions  # 匯入 Mediapipe 的 solutions 模組\n",
    "from mediapipe.framework.formats import landmark_pb2  # 處理關鍵點座標格式\n",
    "import numpy as np  # 用於數值運算\n",
    "import mediapipe as mp  # MediaPipe 核心函式庫\n",
    "from mediapipe.tasks import python  # MediaPipe 任務模組\n",
    "from mediapipe.tasks.python import vision  # MediaPipe 視覺模組\n",
    "import cv2  # OpenCV 用於影像處理\n",
    "\n",
    "# 建立 MediaPipe 臉部偵測器 (FaceLandmarker)\n",
    "base_options = python.BaseOptions(\n",
    "    model_asset_path='models/face_landmarker.task')  # 設定模型路徑\n",
    "options = vision.FaceLandmarkerOptions(\n",
    "    base_options=base_options,\n",
    "    output_face_blendshapes=True,  # 啟用 Blendshapes 偵測\n",
    "    output_facial_transformation_matrixes=True,  # 啟用 3D 變形矩陣輸出\n",
    "    num_faces=1  # 限制偵測 1 張臉\n",
    ")\n",
    "detector = vision.FaceLandmarker.create_from_options(options)  # 初始化偵測器\n",
    "# 讀取圖片並進行臉部偵測\n",
    "image = mp.Image.create_from_file(\"images/face.jpg\")  # 載入影像\n",
    "detection_result = detector.detect(image)  # 執行偵測\n",
    "# 取得臉部特徵點資料\n",
    "face_landmarks_list = detection_result.face_landmarks  # 取得臉部特徵點資料\n",
    "# 判斷是否有偵測到臉部\n",
    "if face_landmarks_list:\n",
    "    annotated_image = np.copy(image.numpy_view())  # 複製影像，避免修改原圖\n",
    "    # 遍歷所有偵測到的臉部 (支援多張臉)\n",
    "    for idx in range(len(face_landmarks_list)):\n",
    "        face_landmarks = face_landmarks_list[idx]  # 取得第 idx 張臉的特徵點\n",
    "        # 建立 MediaPipe 標準的 Landmark 格式\n",
    "        face_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "        face_landmarks_proto.landmark.extend([\n",
    "            landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in face_landmarks\n",
    "        ])\n",
    "        # 在影像上繪製臉部網格 (Tessellation)\n",
    "        solutions.drawing_utils.draw_landmarks(\n",
    "            image=annotated_image,\n",
    "            landmark_list=face_landmarks_proto,\n",
    "            connections=mp.solutions.face_mesh.FACEMESH_TESSELATION,\n",
    "            landmark_drawing_spec=None,\n",
    "            connection_drawing_spec=mp.solutions.drawing_styles.get_default_face_mesh_tesselation_style()\n",
    "        )\n",
    "        # 在影像上繪製臉部輪廓 (Contours)\n",
    "        solutions.drawing_utils.draw_landmarks(\n",
    "            image=annotated_image,\n",
    "            landmark_list=face_landmarks_proto,\n",
    "            connections=mp.solutions.face_mesh.FACEMESH_CONTOURS,\n",
    "            landmark_drawing_spec=None,\n",
    "            connection_drawing_spec=mp.solutions.drawing_styles.get_default_face_mesh_contours_style()\n",
    "        )\n",
    "        # 在影像上繪製虹膜 (Irises)\n",
    "        solutions.drawing_utils.draw_landmarks(\n",
    "            image=annotated_image,\n",
    "            landmark_list=face_landmarks_proto,\n",
    "            connections=mp.solutions.face_mesh.FACEMESH_IRISES,\n",
    "            landmark_drawing_spec=None,\n",
    "            connection_drawing_spec=mp.solutions.drawing_styles.get_default_face_mesh_iris_connections_style()\n",
    "        )\n",
    "\n",
    "# 使用 OpenCV 顯示標記後的影像\n",
    "cv2.imshow(\"MediaPipe FaceMesh\", cv2.cvtColor(\n",
    "    annotated_image, cv2.COLOR_RGB2BGR))\n",
    "cv2.waitKey(0)  # 等待按鍵以關閉視窗\n",
    "cv2.destroyAllWindows()  # 關閉所有 OpenCV 視窗\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2bcbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mediapipe import solutions  # 匯入 Mediapipe 的 solutions 模組\n",
    "from mediapipe.framework.formats import landmark_pb2  # 處理關鍵點座標格式\n",
    "import numpy as np  # 用於數值運算\n",
    "import mediapipe as mp  # MediaPipe 核心函式庫\n",
    "from mediapipe.tasks import python  # MediaPipe 任務模組\n",
    "from mediapipe.tasks.python import vision  # MediaPipe 視覺模組\n",
    "import cv2  # OpenCV 用於影像處理\n",
    "\n",
    "# 設定模型路徑和偵測參數，允許最多 2 隻手\n",
    "base_options = python.BaseOptions(\n",
    "    model_asset_path='models/hand_landmarker.task')  # 設定模型路徑\n",
    "options = vision.HandLandmarkerOptions(\n",
    "    base_options=base_options, num_hands=2)  # 設定偵測參數，最多允許2隻手\n",
    "detector = vision.HandLandmarker.create_from_options(options)  # 建立手部標記偵測器\n",
    "# 載入輸入影像\n",
    "image = mp.Image.create_from_file(\n",
    "    \"images/hands.jpg\")  # 從檔案讀取影像並轉換為 Mediapipe 影像格式\n",
    "# 偵測影像中的手部標記點\n",
    "detection_result = detector.detect(image)  # 使用偵測器對影像進行手部標記偵測\n",
    "# 處理偵測結果\n",
    "if len(detection_result.hand_landmarks) > 0:  # 如果偵測到手部標記點\n",
    "    annotated_image = np.copy(image.numpy_view())  # 建立影像副本\n",
    "    for hand_landmarks in detection_result.hand_landmarks:  # 遍歷所有偵測到的手部標記點\n",
    "        hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()  # 建立 Mediapipe 格式的標記點資料結構\n",
    "        hand_landmarks_proto.landmark.extend([\n",
    "            landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in hand_landmarks\n",
    "        ])  # 填充標記點資料\n",
    "        solutions.drawing_utils.draw_landmarks(\n",
    "            annotated_image,  # 要繪製的影像\n",
    "            hand_landmarks_proto,  # 手部標記點資料\n",
    "            solutions.hands.HAND_CONNECTIONS,  # 預設的手部標記點連線規則\n",
    "            solutions.drawing_styles.get_default_hand_landmarks_style()  # 預設的標記點樣式\n",
    "        )  # 使用 Mediapipe 的繪圖工具繪製手部標記點與連接線\n",
    "\n",
    "cv2.imshow(\"MediaPipe Hands\", cv2.cvtColor(annotated_image,\n",
    "           cv2.COLOR_RGB2BGR))  # 使用 OpenCV 顯示影像，轉換為 BGR 色彩模式\n",
    "cv2.waitKey(0)  # 等待使用者按下任意鍵後關閉視窗\n",
    "cv2.destroyAllWindows()  # 關閉所有 OpenCV 視窗\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af2569fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\q5j6j\\miniconda3\\envs\\CV_mp\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "偵測到的姿勢數量: 1\n"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp  # 匯入 Mediapipe 核心函式庫\n",
    "from mediapipe import solutions  # 匯入 Mediapipe 的 solutions 模組\n",
    "from mediapipe.framework.formats import landmark_pb2  # 匯入 Mediapipe 的關鍵點格式\n",
    "from mediapipe.tasks import python  # 匯入 Mediapipe 的 Python 任務 API\n",
    "from mediapipe.tasks.python import vision  # 匯入 Mediapipe 的視覺處理模組\n",
    "import numpy as np  # 匯入 NumPy 進行影像處理\n",
    "import cv2  # 匯入 OpenCV 進行影像顯示\n",
    "\n",
    "# 初始化 Mediapipe 姿勢偵測器\n",
    "base_options = python.BaseOptions(\n",
    "    model_asset_path='models/pose_landmarker_full.task')  # 設定模型路徑\n",
    "options = vision.PoseLandmarkerOptions(\n",
    "    base_options=base_options, output_segmentation_masks=True)  # 設定偵測參數\n",
    "detector = vision.PoseLandmarker.create_from_options(options)  # 建立姿勢偵測器\n",
    "# 讀取輸入影像\n",
    "mp_image = mp.Image.create_from_file(\n",
    "    \"images/pose.jpg\")  # 從檔案讀取影像並轉換為 Mediapipe 影像格式\n",
    "# 進行姿勢偵測\n",
    "detection_result = detector.detect(mp_image)  # 使用偵測器對影像進行分析\n",
    "# 處理偵測結果\n",
    "if len(detection_result.pose_landmarks) > 0:  # 如果偵測到姿勢關鍵點\n",
    "    print(\"偵測到的姿勢數量:\", len(detection_result.pose_landmarks))  # 印出偵測到的姿勢數量\n",
    "    annotated_image = np.copy(mp_image.numpy_view())  # 建立影像副本\n",
    "    for pose_landmarks in detection_result.pose_landmarks:  # 遍歷所有偵測到的姿勢關鍵點\n",
    "        pose_landmarks_proto = landmark_pb2.NormalizedLandmarkList()  # 建立 Mediapipe 格式的關鍵點資料結構\n",
    "        pose_landmarks_proto.landmark.extend([\n",
    "            landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in pose_landmarks\n",
    "        ])  # 填充關鍵點資料\n",
    "        solutions.drawing_utils.draw_landmarks(\n",
    "            annotated_image,  # 要繪製的影像\n",
    "            pose_landmarks_proto,  # 姿勢關鍵點資料\n",
    "            solutions.pose.POSE_CONNECTIONS,  # 預設的姿勢關鍵點連線規則\n",
    "            solutions.drawing_styles.get_default_pose_landmarks_style()  # 預設的關鍵點樣式\n",
    "        )  # 使用 Mediapipe 的繪圖工具繪製姿勢關鍵點與連接線\n",
    "\n",
    "# 使用 OpenCV 顯示影像，轉換為 BGR 色彩模式\n",
    "cv2.imshow('Image', cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))\n",
    "cv2.waitKey(0)  # 等待使用者按下任意鍵後關閉視窗\n",
    "cv2.destroyAllWindows()  # 關閉所有 OpenCV 視窗\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a70cfc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.6 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\q5j6j\\miniconda3\\envs\\CV_mp\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\q5j6j\\miniconda3\\envs\\CV_mp\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\q5j6j\\miniconda3\\envs\\CV_mp\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 758, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\q5j6j\\miniconda3\\envs\\CV_mp\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\q5j6j\\miniconda3\\envs\\CV_mp\\Lib\\asyncio\\base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\q5j6j\\miniconda3\\envs\\CV_mp\\Lib\\asyncio\\base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\q5j6j\\miniconda3\\envs\\CV_mp\\Lib\\asyncio\\events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\q5j6j\\miniconda3\\envs\\CV_mp\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 701, in shell_main\n",
      "    await self.dispatch_shell(msg, subshell_id=subshell_id)\n",
      "  File \"c:\\Users\\q5j6j\\miniconda3\\envs\\CV_mp\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 469, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\q5j6j\\miniconda3\\envs\\CV_mp\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 379, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\q5j6j\\miniconda3\\envs\\CV_mp\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 899, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\q5j6j\\miniconda3\\envs\\CV_mp\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 471, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\q5j6j\\miniconda3\\envs\\CV_mp\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 632, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\q5j6j\\miniconda3\\envs\\CV_mp\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3116, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\q5j6j\\miniconda3\\envs\\CV_mp\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3171, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\q5j6j\\miniconda3\\envs\\CV_mp\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\q5j6j\\miniconda3\\envs\\CV_mp\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3394, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\q5j6j\\miniconda3\\envs\\CV_mp\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3639, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\q5j6j\\miniconda3\\envs\\CV_mp\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\q5j6j\\AppData\\Local\\Temp\\ipykernel_13732\\260257681.py\", line 14, in <module>\n",
      "    interpreter = Interpreter(model_path)\n",
      "  File \"c:\\Users\\q5j6j\\miniconda3\\envs\\CV_mp\\Lib\\site-packages\\tflite_runtime\\interpreter.py\", line 464, in __init__\n",
      "    self._interpreter = _interpreter_wrapper.CreateWrapperFromFile(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: _ARRAY_API not found"
     ]
    },
    {
     "ename": "SystemError",
     "evalue": "<built-in method CreateWrapperFromFile of PyCapsule object at 0x000001FDD373CDE0> returned a result with an exception set",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[31mImportError\u001b[39m: numpy.core.multiarray failed to import",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mSystemError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f.readlines():\n\u001b[32m     13\u001b[39m         label_names.append(line.strip())\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m interpreter = \u001b[43mInterpreter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m成功載入模型...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m interpreter.allocate_tensors()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\q5j6j\\miniconda3\\envs\\CV_mp\\Lib\\site-packages\\tflite_runtime\\interpreter.py:464\u001b[39m, in \u001b[36mInterpreter.__init__\u001b[39m\u001b[34m(self, model_path, model_content, experimental_delegates, num_threads, experimental_op_resolver_type, experimental_preserve_all_tensors, experimental_disable_delegate_clustering)\u001b[39m\n\u001b[32m    458\u001b[39m custom_op_registerers_by_name = [\n\u001b[32m    459\u001b[39m     x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._custom_op_registerers \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mstr\u001b[39m)\n\u001b[32m    460\u001b[39m ]\n\u001b[32m    461\u001b[39m custom_op_registerers_by_func = [\n\u001b[32m    462\u001b[39m     x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._custom_op_registerers \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mstr\u001b[39m)\n\u001b[32m    463\u001b[39m ]\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m \u001b[38;5;28mself\u001b[39m._interpreter = \u001b[43m_interpreter_wrapper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCreateWrapperFromFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m    \u001b[49m\u001b[43mop_resolver_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_op_registerers_by_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_op_registerers_by_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexperimental_preserve_all_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexperimental_disable_delegate_clustering\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._interpreter:\n\u001b[32m    473\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mFailed to open \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m'\u001b[39m.format(model_path))\n",
      "\u001b[31mSystemError\u001b[39m: <built-in method CreateWrapperFromFile of PyCapsule object at 0x000001FDD373CDE0> returned a result with an exception set"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from tflite_runtime.interpreter import Interpreter\n",
    "except ImportError:\n",
    "    from tensorflow.lite.python.interpreter import Interpreter\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "model_path = \"models/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite\"\n",
    "label_path = \"models/labels.txt\"\n",
    "label_names = []\n",
    "with open(label_path, \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        label_names.append(line.strip())\n",
    "interpreter = Interpreter(model_path)\n",
    "print(\"成功載入模型...\")\n",
    "interpreter.allocate_tensors()\n",
    "_, height, width, _ = interpreter.get_input_details()[0][\"shape\"]\n",
    "print(\"影像尺寸: (\", width, \",\", height, \")\")\n",
    "image = cv2.imread(\"images/dog.jpg\")\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "image_resized = cv2.resize(image_rgb, (width, height))\n",
    "input_data = np.expand_dims(image_resized, axis=0)\n",
    "interpreter.set_tensor(\n",
    "    interpreter.get_input_details()[0][\"index\"], input_data)\n",
    "interpreter.invoke()\n",
    "output_details = interpreter.get_output_details()[0]\n",
    "output = np.squeeze(interpreter.get_tensor(output_details[\"index\"]))\n",
    "label_id = np.argmax(output)\n",
    "scale, zero_point = output_details[\"quantization\"]\n",
    "prob = scale * (output[label_id] - zero_point)\n",
    "classification_label = label_names[label_id]\n",
    "print(\"分類名稱 =\", classification_label)\n",
    "print(\"影像可能性 =\", np.round(prob*100, 2), \"%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CV_mp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
